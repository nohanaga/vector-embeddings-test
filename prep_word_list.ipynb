{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word List 1000\n",
    "\n",
    "元の文章テキストファイルから単語を 1,000 個抽出する。\n",
    "用途に合わせて2種類の形態素解析器を使用することができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_all_txt_files(directory):\n",
    "    combined_text = \"\"\n",
    "    # ディレクトリ内の全てのファイルを走査\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            # 拡張子が.mdのファイルのみ対象\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    combined_text += f.read() + \"\\n\"  # ファイル内容を結合\n",
    "    return combined_text\n",
    "\n",
    "# 使用例\n",
    "directory_path = \"./data\"  # ディレクトリのパスを指定\n",
    "all_text = read_all_txt_files(directory_path)\n",
    "\n",
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# JSONファイルからストップワードを読み込む\n",
    "with open(\"./custom_stopwords_ja.json\", encoding=\"utf-8\") as f:\n",
    "    stopwords = set(json.load(f)[\"stopwords\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sudachi Extractor\n",
    "https://github.com/WorksApplications/SudachiPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sudachipy\n",
    "from sudachipy import dictionary\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "# 形態素解析器のインスタンス作成\n",
    "tokenizer = dictionary.Dictionary().create()\n",
    "\n",
    "# 名詞カウンター\n",
    "noun_counter = Counter()\n",
    "\n",
    "# 名詞（普通名詞）の抽出\n",
    "for line in all_text.split('\\n'):\n",
    "    if line.strip():\n",
    "        for morpheme in tokenizer.tokenize(line, mode=tokenizer.SplitMode.C):\n",
    "            pos = morpheme.part_of_speech()\n",
    "\n",
    "            # 「名詞」「普通名詞」のチェック\n",
    "            if pos[0] == \"名詞\" and pos[1] == \"普通名詞\":\n",
    "                word = morpheme.surface()\n",
    "                if not re.fullmatch(r'^[a-zA-Z0-9]+$', word) and word not in stopwords:\n",
    "                    noun_counter[word] += 1\n",
    "\n",
    "            # 1000個集まったら終了\n",
    "            if len(noun_counter) >= 1000:\n",
    "                break\n",
    "\n",
    "# 上位1000個の名詞を取得\n",
    "top_nouns = noun_counter.most_common(1000)\n",
    "\n",
    "# データフレームに変換\n",
    "df = pd.DataFrame(top_nouns, columns=['Noun', 'Frequency'])\n",
    "\n",
    "# CSVとして保存\n",
    "df.to_csv('./embeddings/busho_nouns_sudachi_1000.csv', index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MeCab + Neologd\n",
    "\n",
    "https://github.com/neologd/mecab-ipadic-neologd/blob/master/README.ja.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import MeCab\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# MeCabの形態素解析器を生成（NEologd辞書を指定）\n",
    "mecab = MeCab.Tagger(\"-d /opt/homebrew/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "\n",
    "# 名詞カウンター\n",
    "noun_counter = Counter()\n",
    "\n",
    "# サンプルテキスト\n",
    "# sample_text = \"\"\"\n",
    "# 私たちはAI技術を使って新しいソリューションを開発しています。\n",
    "# 東京都内の企業が集まるイベントでプレゼンテーションを行いました。\n",
    "# スポーツ、文化、教育、科学、技術など、さまざまな分野における研究が進んでいます。\n",
    "# 社会問題解決のために、地域社会との連携が重要です。\n",
    "# 最新技術を用いた製品開発が進行中です。PythonやJavaScriptの勉強も必要です。\n",
    "# \"\"\"\n",
    "\n",
    "# 名詞抽出とストップワード除去\n",
    "for line in all_text.split('\\n'):\n",
    "    if line.strip():\n",
    "        parsed = mecab.parse(line)\n",
    "        for row in parsed.split('\\n'):\n",
    "            if row == 'EOS' or row == '':\n",
    "                continue\n",
    "            cols = row.split('\\t')\n",
    "            if len(cols) > 1:\n",
    "                word = cols[0]\n",
    "                pos = cols[1].split(',')\n",
    "\n",
    "                # 名詞かつストップワードでないものを抽出\n",
    "                # and pos[1] == \"固有名詞\" でもいい\n",
    "                if pos[0] == \"名詞\"  and word not in stopwords:\n",
    "                    # 英字や数字、%を含まないものを除外\n",
    "                    if not re.fullmatch(r'^[a-zA-Z0-9%]+$', word) and not re.fullmatch(r'.*\\d+.*', word) and not re.fullmatch(r'^[a-zA-Z\\W]+$', word):\n",
    "                        noun_counter[word] += 1\n",
    "\n",
    "            # 1000個集まったら終了\n",
    "            if len(noun_counter) >= 5000:\n",
    "                break\n",
    "\n",
    "# 上位1000個の名詞を取得\n",
    "top_nouns = noun_counter.most_common(1000)\n",
    "\n",
    "# データフレームに変換\n",
    "df = pd.DataFrame(top_nouns, columns=['Noun', 'Frequency'])\n",
    "\n",
    "# CSVとして保存\n",
    "df.to_csv('./embeddings/busho_nouns_neologd_1000.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py1226",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
